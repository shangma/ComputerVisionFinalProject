%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
600.461 Computer Vision\\Final Report
}

\author{Noah Belcher and Juneki Hong and Roger Xu% <-this % stops a space
\thanks{Noah Belcher,
        {\tt\small nbelche1@jhu.edu}}%
\thanks{Juneki Hong,
        {\tt\small jhong29@jhu.edu}}%
\thanks{Roger Xu,
		{\tt\small rxu8@jhu.edu}}%
}



\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\section{INTRODUCTION}

Our group chose to work on “Object Detection and Classification in a Video Sequence” as the final project for this course. The state purpose of this project was: given a video of a traffic intersection, recognize and classify the moving objects (cars, taxis, motorbikes, bicycles, pedestrians, etc). The final objective of this project was to produce a video in which these moving objects are identified and classified by bounding boxes. There were several challenges involved with this project, including but not limited to: shakiness of the video, motion detection, tracking, and classification. In this paper, we will summarize the approaches that we took towards dealing with these challenges and working towards our final objective. \newline


In order to succesfully do this project, we broke it down into basic components of image detection, image categorization, and image tracking, and we created implementations for each of these steps. With everything put together, we produced some decent results. We were able to identify and even do some tracking on the objects that appear throughout the video. 


\hfill December 2, 2011


\section{METHODS USED}

\subsection{Background Subtraction}
The first obvious step in this project was to implement some sort of background subtraction. Doing so would provide the benefit of being able to clearly identify the moving objects, which would provide the baseline for all the subsequent steps. \newline

We first took a rather basic approach: iterate through all of the frames and compute either a median or mean image, which would serve as the background in our background subtraction. Even after background stabilization, we would find that the mean image was rather blurry, so we used the median image. Apart from a the thin smear of cars across the road, the median image would look almost like an empty street. \newline

In any case, the naive background without stabilization was really blurry; the edges were not clear because we tried to compute the background on frames that moved over time. \newline

Nonetheless, we continued in our approach and performed a background subtraction on every image. The resulting video showed some promise. The video contained some unexpected parts of the image that were not supposed to be there (buildings, crosswalks, and other non-moving objects), but succeeded in picking up all the moving objects and removing much of the non-important parts of the image. \newline


\subsubsection{Image Stabilization}

To deal with the shakiness of the video, we then decided to stabilize the video prior to performing background subtraction. To do this, we implemented methods similar to those used in assignment two. First we computed the SIFT features for each of the images. Then computed a homography that corresponds the SIFT features of image i to the SIFT features of image 1. Finally, use the computed homography to warp image i onto the space of image 1. \newline

The resulting images were such that each image adjusted itself so as to give the illusion that the camera is stable, and in most of these images, there were some black edges caused by the warping. Once we had a directory of stable images, we again performed background subtraction, this time using the stable images as opposed to the actual images of the original video. The result was much improved. There was significantly less exposure of static objects. \newline

Still, however, the video was not quite perfect; although noise from the buildings and other static objects was significantly reduced, their mere presence would be a threat to object detection and tracking. To accommodate for the major sources of static object exposure, we applied a mask to parts of the video that we deemed to be unnecessary. This included the buildings in the background, the street crosswalk lines, and the lone taxi that is parked. After we did this, we decided that the resulting video was acceptable to perform detection and tracking on.\newline

\subsubsection{Object Detection}



////////////////////////////////////////////////////////////////




\subsection{Categorization}

Once we have a list of detected objects, we can then try to classify them. We used a simple method of color matching for our classification. We have a series of “training data” to be fed to our program. This training data serves the purpose of teaching our program what an object can be classified as. \newline

Statistical analysis is performed for each class of training data. For each different type of object in the training data (Taxis, cars, pedestrians, etc), we take each RGB sample image and compute the mean R, G, and B values and standard deviations. For example, taxis seem to have about an average of 127, 132, and 111 RGB mean values with standard deviations of 13, 12, and 12 respectively. \newline

Once we have "trained" and have this information in place, we start classifying the detected objects. For example, if a detected object was within a threshold of about a standard deviation for an object, then we could classify it as that object. Note that we only have training data for white cars, taxis, and bicycles. We left out pedestrians from the training data because of the amount of variation that has to be accounted for: shirt color, hair color, skin color, etc. \newline

Also, we decided that anything that is not a taxi, car, or bicycle would be classified as pedestrian. We admit, this was a rather convienent approach that we took, because the pedestrians had somewhat little distinguishing color features. There were times when the pedestrians looked like a small blob of dark pixels. They would show up in the background subtraction images, but they did not have a very distinguishing RGB pattern.\newline





\subsection{Tracking}

We came up with a relatively simple tracking algorithm to use on the moving objects in the video. It basically consists of three steps: First, we detect every possible object on the image. Second, there is a "tracking" step to update and maintain every object that we are already tracking. And finally third, there is a "registration" step for every object that is detected but is not a tracked object. 
\newline


\subsubsection{Detection Step}

For a given image, the first thing we do is run our object detection code to find all of the potential moving objects in the frame. In the end, this will generate a set of coordinates and window sizes of possible objects. 
\newline


\subsubsection{Tracking Step}

We take the set of detected objects and check with our maintained list of tracked objects. The matrix containing all of our tracked objects stores and keeps track of the previous two coordinates of the object, the size of the window around the object, and the classification of the object. We compute the predicted coordinate of where we expect the tracked object should be in this frame. \newline

For each tracked object, we check if there exists a detected object that is really close to this predicted location. We do this by iterating through all of the detected object coordinates, computing each distance to our tracked object, choosing the point with the minimum distance, and then looking to see if this distance is below a threshold. If we believe that a detected object matches a tracked object, we update the coordinates stored in the tracked object and we delete the detected object from the set. \newline

If no detected objects are near a tracked object, we then scan around the predicted location, looking for the tracked object just in case. Our detection algorithm relied on the data generated from the background subtraction, and so if a vehicle stops in the middle of the road, we wouldn't be able to detect it anymore. Using the window size and the classification of the object, we look to see if the object can still be detected using code inspired from our RGB classification program. \newline

If we manage to find the object again this way, we then update the coordinates of the tracked object and move on. If we cannot find the object, we then delete the object from our list of tracked objects. \newline 

\subsubsection{Registration Step}

By this point, some of the detected objects should have been deleted because we determined that they were associated with a tracked object. We deal with the remaining detected objects by registering them. \newline

Similar to the tracked objects, we have a list of registered objects that we are trying to track for a short period of time before we are confident enough that these are not noise, but objects worthy of being tracked. For each registered object, we also iterate over the list of detected objects, trying to determine if there exists a detected object that is close to a registered object. If we find a match, we then update the registered object (its coordinates and the number of times we have detected it) and then delete the corresponding detected object. \newline

At the end, we iterate over our registered objects. We decrement any object that had not been updated in this step. If any of the counters arrive at zero at this point, it is deleted from the list (it must have been random noise or something). If any of them had been detected three or more times by this point, we move the object from the registered list to the tracking list. Any remaining detected objects at this point are added to the registered list. \newline

\section{RESULTS}


\section{CONCLUSION}


\section{CODE INFORMATION}



--------------------------------------------------------

--- The old stuff is below this line

--------------------------------------------------------





\section{CONCLUSION}
So far, we have only gotten background subtraction working and we also have some functionality for image stabilization. Ahead of us, we will still need to do object detection, tracking, and classification, but I think we do have an idea of how we are going to go about accomplishing these tasks. 













\begin{thebibliography}{99}

\bibitem{c1}
Vedaldi, Andrea, and Brian Fulkerson. {\it VLFeat}, http://www.vlfeat.org/.



\end{thebibliography}

\end{document}
